# Masked Language Modeling

## Models

| Name                                         | Description                                                                                                                                                                                                          | Author                       | Link                                                                          |
|----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------|-------------------------------------------------------------------------------|
| IndoConvBERT Base Model                      | IndoConvBERT is a ConvBERT model pretrained on Indo4B.                                                                                                                                                               | Akmal                        | [HuggingFace](https://huggingface.co/Wikidepia/IndoConvBERT-base)             |
| Indonesian BERT Base 1.5G (uncased)          | It is BERT-base model pre-trained with indonesian Wikipedia and indonesian newspapers using a masked language modeling (MLM) objective. This model is uncased.                                                       | Cahya Wirawan                | [HuggingFace](https://huggingface.co/cahya/bert-base-indonesian-1.5G)         |
| Indonesian BERT Base 522M (uncased)          | It is BERT-base model pre-trained with indonesian Wikipedia using a masked language modeling (MLM) objective. This model is uncased: it does not make a difference between indonesia and Indonesia.                  | Cahya Wirawan                | [HuggingFace](https://huggingface.co/cahya/bert-base-indonesian-522M)         |
| Indonesian RoBERTa Base 522M (uncased)       | It is RoBERTa-base model pre-trained with indonesian Wikipedia using a masked language modeling (MLM) objective. This model is uncased: it does not make a difference between indonesia and Indonesia.               | Cahya Wirawan                | [HuggingFace](https://huggingface.co/cahya/roberta-base-indonesian-522M)      |
| Indonesian DistilBERT Base (uncased)         | This model is a distilled version of the Indonesian BERT base model. This model is uncased. This is one of several other language models that have been pre-trained with indonesian datasets.                        | Cahya Wirawan                | [HuggingFace](https://huggingface.co/cahya/distilbert-base-indonesian)        |
| IndoELECTRA                                  | IndoELECTRA is a pre-trained language model based on ELECTRA architecture for the Indonesian Language. This model is base version which use electra-base config.                                                     | Christopher Albert Lorentius | [HuggingFace](https://huggingface.co/ChristopherA08/IndoELECTRA)              |
| Indonesian RoBERTa Base                      | Indonesian RoBERTa Base is a masked language model based on the RoBERTa model. It was trained on the OSCAR dataset, specifically the unshuffled_deduplicated_id subset.                                              | Flax Community               | [HuggingFace](https://huggingface.co/flax-community/indonesian-roberta-base)  |
| Indonesian RoBERTa Large                     | Indonesian RoBERTa Large is a masked language model based on the RoBERTa model. It was trained on the OSCAR dataset, specifically the unshuffled_deduplicated_id subset.                                             | Flax Community               | [HuggingFace](https://huggingface.co/flax-community/indonesian-roberta-large) |
| IndoBERT Base Model (phase1 - uncased)       | IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. | Indo Benchmark               | [HuggingFace](https://huggingface.co/indobenchmark/indobert-base-p1)          |
| IndoBERT Base Model (phase2 - uncased)       | IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. | Indo Benchmark               | [HuggingFace](https://huggingface.co/indobenchmark/indobert-base-p2)          |
| IndoBERT Large Model (phase1 - uncased)      | IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. | Indo Benchmark               | [HuggingFace](https://huggingface.co/indobenchmark/indobert-large-p1)         |
| IndoBERT Large Model (phase2 - uncased)      | IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. | Indo Benchmark               | [HuggingFace](https://huggingface.co/indobenchmark/indobert-large-p2)         |
| IndoBERT-Lite Base Model (phase1 - uncased)  | IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. | Indo Benchmark               | [HuggingFace](https://huggingface.co/indobenchmark/indobert-lite-base-p1)     |
| IndoBERT-Lite Base Model (phase2 - uncased)  | IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. | Indo Benchmark               | [HuggingFace](https://huggingface.co/indobenchmark/indobert-lite-base-p2)     |
| IndoBERT-Lite Large Model (phase1 - uncased) | IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. | Indo Benchmark               | [HuggingFace](https://huggingface.co/indobenchmark/indobert-lite-large-p1)    |
| IndoBERT-Lite Large Model (phase2 - uncased) | IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. | Indo Benchmark               | [HuggingFace](https://huggingface.co/indobenchmark/indobert-lite-large-p2)    |
| IndoBERT Base (uncased)                      | IndoBERT is the Indonesian version of BERT model. The model was trained using over 220M words, aggregated from three main sources: Indonesian Wikipedia, news articles, and an Indonesian Web Corpus.                | IndoLEM                      | [HuggingFace](https://huggingface.co/indolem/indobert-base-uncased)           |
| IndoBERT (Indonesian BERT Model)             | IndoBERT is a pre-trained language model based on BERT architecture for the Indonesian Language. This model is base-uncased version which use bert-base config.                                                      | Sarah Lintang                | [HuggingFace](https://huggingface.co/sarahlintang/IndoBERT)                   |
| Indo RoBERTa Small                           | Indo RoBERTa Small is a masked language model based on the RoBERTa model. It was trained on the latest (late December 2020) Indonesian Wikipedia articles.                                                           | Wilson Wongso                | [HuggingFace](https://huggingface.co/w11wo/indo-roberta-small)                |