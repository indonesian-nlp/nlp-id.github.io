# Masked Language Modeling

## Models

| Name                                       | Description                                                                                                                                                                                           | Author        | Link                                                                   |
| ------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- | ---------------------------------------------------------------------- |
| IndoBERT                                   | IndoBERT is the Indonesian version of BERT model. The model was trained using over 220M words, aggregated from three main sources: Indonesian Wikipedia, news articles, and an Indonesian Web Corpus. | IndoLEM       | [HuggingFace](https://huggingface.co/indolem/indobert-base-uncased)    |
| Indonesian DistilBERT base model (uncased) | This model is a distilled version of the Indonesian BERT base model. This model is uncased. This is one of several other language models that have been pre-trained with indonesian datasets.         | Cahya Wirawan | [HuggingFace](https://huggingface.co/cahya/distilbert-base-indonesian) |
