# Masked Language Modeling

## Models

| Name                                   | Description                                                                                                                                                                                            | Author         | Link                                                                          |
| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------- | ----------------------------------------------------------------------------- |
| IndoBERT Base (uncased)                | IndoBERT is the Indonesian version of BERT model. The model was trained using over 220M words, aggregated from three main sources: Indonesian Wikipedia, news articles, and an Indonesian Web Corpus.  | IndoLEM        | [HuggingFace](https://huggingface.co/indolem/indobert-base-uncased)           |
| Indonesian DistilBERT Base (uncased)   | This model is a distilled version of the Indonesian BERT base model. This model is uncased. This is one of several other language models that have been pre-trained with indonesian datasets.          | Cahya Wirawan  | [HuggingFace](https://huggingface.co/cahya/distilbert-base-indonesian)        |
| Indonesian BERT Base 522M (uncased)    | It is BERT-base model pre-trained with indonesian Wikipedia using a masked language modeling (MLM) objective. This model is uncased: it does not make a difference between indonesia and Indonesia.    | Cahya Wirawan  | [HuggingFace](https://huggingface.co/cahya/bert-base-indonesian-522M)         |
| Indonesian BERT Base 1.5G (uncased)    | It is BERT-base model pre-trained with indonesian Wikipedia and indonesian newspapers using a masked language modeling (MLM) objective. This model is uncased.                                         | Cahya Wirawan  | [HuggingFace](https://huggingface.co/cahya/bert-base-indonesian-1.5G)         |
| Indonesian RoBERTa Base 522M (uncased) | It is RoBERTa-base model pre-trained with indonesian Wikipedia using a masked language modeling (MLM) objective. This model is uncased: it does not make a difference between indonesia and Indonesia. | Cahya Wirawan  | [HuggingFace](https://huggingface.co/cahya/roberta-base-indonesian-522M)      |
| Indonesian RoBERTa Large               | Indonesian RoBERTa Large is a masked language model based on the RoBERTa model. It was trained on the OSCAR dataset, specifically the unshuffled_deduplicated_id subset.                               | Flax Community | [HuggingFace](https://huggingface.co/flax-community/indonesian-roberta-large) |
| Indonesian RoBERTa Base                | Indonesian RoBERTa Base is a masked language model based on the RoBERTa model. It was trained on the OSCAR dataset, specifically the unshuffled_deduplicated_id subset.                                | Flax Community | [HuggingFace](https://huggingface.co/flax-community/indonesian-roberta-base)  |
| Indo RoBERTa Small                     | Indo RoBERTa Small is a masked language model based on the RoBERTa model. It was trained on the latest (late December 2020) Indonesian Wikipedia articles.                                             | Wilson Wongso  | [HuggingFace](https://huggingface.co/w11wo/indo-roberta-small)                |
